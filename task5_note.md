# task_05 分类分析

[教程地址](https://github.com/Git-Model/Modeling-Universe/blob/main/Data%20Analysis%20and%20Statistical%20Modeling/task_05%20%E5%88%86%E7%B1%BB%E5%88%86%E6%9E%90/%E5%88%86%E7%B1%BB.ipynb)

#### 简单总结
* 分类分析
  * 根据目的选择框架
    * 追求预测准确度：深度学习/机器学习模型
    * 追求解释性：线性模型框架  
  * 分类问题与对应模型
    * 二分类问题：可用二分类Logistic回归模型与Probit模型
    * 无序多分类问题：可用多类别logistic回归模型建模
    * 有序多分类问题：有序Logistic回归模型
* 二分类问题
  * 可以看作输出的是 y=1 的概率
  * 线性概率模型(LPM)：跟线性回归公式差不多，但是输出概率
  * 概率的映射：可以使用映射函数——Logistic函数或Probit函数，Probit函数实际上就是标准正态分布的累积函数
  * Logistic线性回归模型
    * $P(y=1 \mid x)=p\left( x \right) =\frac{1}{1+e^{-y}}=\frac{1}{1+e^{-x^{'}\beta}}$
  * Probit线性回归模型
    * $p(x)=\Phi\left(x^{\prime} \beta\right)$，其中，$\Phi$就是标准正态分布的累积函数
  * 模型推断
    * 与OLS估计的多元线性回归不同的是，这两种分类回归模型采用的是极大似然估计法对模型参数进行估计，在极大似然估计的一般理论下，我们可以证明模型估计系数$\hat{\beta}$是$\beta$的一致估计
    * 单参数显著性检验：(公式略)
    * 多参数联合显著性检验
      * 多元线性回归模型不一样的是，分类模型没有残差平方和RSS的概念，那么联合检验的检验统计量自然也不是原来的F统计量了。但是，在似然理论下，存在一个比残差平方和更广泛的概念——离差(Deviance)；当两个模型的离差足够大时，我们便可以拒绝原假设，认为参数是联合显著的（公式略）
  * 分类预测
    * 模型预测的原理
      * 根据公式算概率，大于阈值（一般是 0.5）则预测 y = 1
    * 预测结果呈现——混淆矩阵与多指标
      * 混淆矩阵：两个轴构成的四个区域，一个轴表示是否阳性，另一个轴表示是否预测为阳性
      * 多分类指标——分类精度真的普适吗？
        * 根据多个公式综合判断：精确度计算、精确率、召回率、F分数
        * 如果目标是限制假正例（ST=0被预测为ST=1），那么精确率就可以很好的度量
        * 如果我们想限制假反例（ST=1被预测为ST=0），那么召回率就可以很好的度量
        * F分数(F-score) 是精确率与召回率的调和平均，是两者的综合取舍
      * 对抗不平衡数据集——改变阈值
        * 调整阈值是一把“双刃剑”，对于一个既定的模型，改变阈值并不会改变模型的预测性能，它只能改变样本预测的分布，使分布更符合我们实际的需求。如果需要改进预测效果，要么换一种精度更高的模型，要么进行调参处理
  * 无序多分类问题
    * 由于技术限制，教程只介绍了预测方法，没有介绍推断方法
    * 多分类Logistic回归的两种算法
      * 概率映射函数：将其中一个类作为基类，通过修改过后的映射函数获得k-1个对数胜率
      * 一对其余”(one-vs-rest)：对每个类别都建立一个二分类Logistic回归，将本类别的样本定义为0，其余类别的样本定义为1，在所有类别的分类器中，概率 p 最高的类别就是实际判定的类别
    * 以预测为目的分类问题
      * 可以基于sklearn 很方便地进行 fit 和 predict
      

#### 体会

1. 不错的教程，其实我之前学过斯坦福吴恩达的机器学习课程，但是那门课主要是以预测为目的。现在回过头来学这门课，学完后感觉在模型的可解释性方面的空白得到了填补，可能以后调参更加得心应手了，哈哈
