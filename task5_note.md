# task_05 分类分析

[教程地址](https://github.com/Git-Model/Modeling-Universe/blob/main/Data%20Analysis%20and%20Statistical%20Modeling/task_05%20%E5%88%86%E7%B1%BB%E5%88%86%E6%9E%90/%E5%88%86%E7%B1%BB.ipynb)

#### 简单总结
* 分类分析
  * 根据目的选择框架
    * 追求预测准确度：深度学习/机器学习模型
    * 追求解释性：线性模型框架  
  * 分类问题与对应模型
    * 二分类问题：可用二分类Logistic回归模型与Probit模型
    * 无序多分类问题：可用多类别logistic回归模型建模
    * 有序多分类问题：有序Logistic回归模型
* 二分类问题
  * 可以看作输出的是 y=1 的概率
  * 线性概率模型(LPM)：跟线性回归公式差不多，但是输出概率
  * 概率的映射：可以使用映射函数——Logistic函数或Probit函数，Probit函数实际上就是标准正态分布的累积函数
  * Logistic线性回归模型
    * $P(y=1 \mid x)=p\left( x \right) =\frac{1}{1+e^{-y}}=\frac{1}{1+e^{-x^{'}\beta}}$
  * Probit线性回归模型
    * $p(x)=\Phi\left(x^{\prime} \beta\right)$，其中，$\Phi$就是标准正态分布的累积函数
  * 模型推断
    * 与OLS估计的多元线性回归不同的是，这两种分类回归模型采用的是极大似然估计法对模型参数进行估计，在极大似然估计的一般理论下，我们可以证明模型估计系数$\hat{\beta}$是$\beta$的一致估计
    * 单参数显著性检验：(公式略)
    * 多参数联合显著性检验
      * 多元线性回归模型不一样的是，分类模型没有残差平方和RSS的概念，那么联合检验的检验统计量自然也不是原来的F统计量了。但是，在似然理论下，存在一个比残差平方和更广泛的概念——离差(Deviance)；当两个模型的离差足够大时，我们便可以拒绝原假设，认为参数是联合显著的（公式略）
  * 分类预测
    

#### 体会

1. 