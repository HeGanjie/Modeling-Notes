# task_03 回归分析2_线性回归模型的推断与推广

[教程地址](https://github.com/Git-Model/Modeling-Universe/blob/main/Data%20Analysis%20and%20Statistical%20Modeling/task_03%20%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%902_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%8E%A8%E5%B9%BF/%E5%9B%9E%E5%BD%922.ipynb)

# 简单总结
3. 回归分析的重要任务——推断/假设检验
  * 需要借助概率论与数理统计中假设检验的知识来辨别：一个系数怎样子才算是接近0（影响不显著）
  * t检验
    * 是回归分析中单个线性假设检验问题的常用检验方法
    * 从单参数检验：若 |beta - C| > 0 则说明参数 beta 影响显著
    * 显著性水平：
      * alpha = P(|无偏beta| > C)
      * 即原假设成立但是我们拒绝了它。此类错误的概率称为拒真概率
      * 保证发生此类错误的概率需要在一个给定的、较小的水平 α ，也被称为显著性水平
      * 我理解就是出错的容忍程度
    * 实际计算时先计算$\frac{\hat{\beta}_{j}-\beta_{j}}{\operatorname{se}\left(\hat{\beta}_{j}\right)}$，再与python输出的t分布对应的双侧分位点值$\pm {t_{n-k-1}\left( 1-\frac{\alpha}{2} \right)} $进行比较 （双边检验）
  * 单边检验：
    * 用于判断某某自变量对因变量是否存在正效应影响
  * p 值
    * 是在本次分析的样本观测值下，给出的能拒绝原假设的最小显著性水平，它只与样本观测值和我们做的假设检验有关
    * 越小越可以拒绝原假设
    * 我理解就是参数可忽略的概率
  * 参数线性组合的检验  
    * 解决两个参数比较大小问题：令 β1=θ1+β2，则可把校验问题转换为假设 θ = 0，转换为单参数问题
  * F检验
    * 是回归分析中多个线性假设检验问题的常用检验方法
    * 更像是在比较两种模型的差异程度，原模型和忽略了参数后的模型
    * 思想：一般而言，模型变量越多，对训练集数据的变异解释程度会越高，拟合优度会越好，进而残差平方和会减小。如果两个模型残差平方和的差异足够大，说明原假设约束的加入是模型产生了显著性的变化，这意味着原假设是显著的
  * 更广义的“线性”回归——多种形式自变量  
    * 自变量不一定是 x，可以是 $x^2$、$\log(x)$ 等
    * 所谓线性回归模型，线性关系并不是指代被解释变量 y 与解释变量 X 之间的关系，而是指回归函数相对于回归系数是线性的
  * 带有定性变量的回归分析
    * 二分类变量，我理解就是 OneHotEncode
    * 交互效应模型
      * 想要判断是否存在交互效应：我们需要在原模型的基础上加上自变量的交互乘积项
      * 自变量可以是二分类变量和定量变量
    * 多分类变量
      * 如果一个变量有n个类别，则需要定义n-1个虚拟变量表示它（避免违反 CLM假设中的MLR.4，不能完全共线性）
      * 常用 pandas 包的 get_dummies 函数进行重编码，然后再分析 p 值
  * 带有自变量函数的回归分析
    * 对数化：
      * 对数变换可以方便地计算变换百分比，于“价格”型变量而言，百分比解释比绝对值解释更有经济意义
      * 当因变量为严格取正的变量，它的分布一般存在异方差性或偏态性，这容易违背CLM假设的同方差/正态性假设。而对数变换可以缓和这种情况
      * 注意：存在负值的变量不可以对数变换；其次，当原变量 y 有部分取值位于[0,1]区间时， log(y) 的负数值会非常大！而线性模型对极端值是非常敏感的，这会影响模型的效果
      * 长久的实践中，我们认为可以遵循以下经验：
        * 对于大数值大区间变量（价格类变量、人口变量等），可取对数变换，如：工资、薪水、销售额、企业市值、人口数量、雇员数量等。
        * 对于小数值小区间变量（时间类变量等），一般不取对数变换，如：受教育年限、工作年限、年龄等
    * 二次项
      * 例如 y = x + u  ->  y = x + x^2 + u
      * 如果我们在回归建模前认为某个自变量对因变量的影响不是线性的，可以尝试加入二次项，并观察二次项的显著性。如果显著，就说明两者关系确实为非线性的
    * 交互项
      * 若某一个变量对因变量的效应依赖于另一个自变量，则我们可以在模型中加入交互项，参考上面的交互效应模型

#### 体会

1. 不错的教程，学了之后，感觉对神经网络模型的解释能力有了很大的提高，也大致搞懂了模型瘦身的原理。此外也感觉到数据分析真的博大精深，其实很多细节我都没有完全弄懂，不过一步一步来吧，先尝试在实际项目中用起来  

2. 不知道有没有办法把这些知识用于无监督学习或半监督学习    

