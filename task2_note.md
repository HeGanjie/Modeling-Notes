# task_02 回归分析1_线性回归模型必知必会

[教程地址](https://github.com/Git-Model/Modeling-Universe/blob/main/Data%20Analysis%20and%20Statistical%20Modeling/task_02%20%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%901_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/%E5%9B%9E%E5%BD%921.ipynb)

#### 回归分析1_线性回归模型必知 简单总结
* 回归分析与分类分析都是一种基于统计模型的统计分析方法。它们都研究因变量（被解释变量）与自变量（解释变量）之间存在的潜在关系，并通过统计模型的形式将这些潜在关系进行显式的表达
* 回归分析的过程本质上一种建模过程，统计建模的主要任务：预测与推断
    * 当前具有强大预测性能的模型大多都是黑盒模型，如强大的Xgboost机器学习算法以及各种深度学习算法
    * 推断任务中最常使用的模型正是线性回归模型，因为用户更容易知道模型内各种参数的数值与统计推断性质
* 回归思想与一般回归模型
  * 横截面数据
    * 是回归分析最主要的分析数据类型，它可以视为在同一时间点（或抽样时间差异可以被忽略）上对多个抽样个体的观测数据
    * 最重要的一个特征：可以将采集的数据近似视为来自一个潜在总体的随机样本，假设我们进行数据分析的最终目的是为了找到 x 与 y 之间的关系并用模型显性表示出来，此时最理想的状态是使用一个条件分布刻画 x 对 y 的影响
    * 在实际问题中，直接估计这个条件分布几乎是一件不可能的事，且我们也难以对分布进行解释与应用。于是，我们退而求其次通过分布的一般数字特征对两者的关系进行推断，如条件分布的中心位置，形状，即考虑**条件均值、条件方差**
    * 回归建模的本质也正是“条件均值的建模”：只要假定随机误差 u 与 x 不相关（这里可理解为其他影响 y 的外生因素与内生因素 x 不相关），我们就可以根据需要假定回归模型的具体形式
  * 时间序列数据：单个个体在不同时间点上的观测数据
  * 面板数据：多个个体在不同时间点上的观测数据
  * 线性回归模型
    * 线性回归模型是最常用假定形式，也是回归分析中最重要的模型
    * 线性回归模型可表示为：
      $$
      y=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{p} x_{p}+u, \quad E\left(u \mid x_{1}, \cdots, x_{p}\right)=0
      $$
    * $\beta_i=\frac{\partial m\left( x \right)}{\partial x_i}$是回归函数对变量$x_i$的偏导数，它被解释为**在保持其他自变量不变的情况下，$x_i$每增加一单位，$y$平均增加$\beta_i$个单位**
* 模型系数的估计方法——OLS估计及其性质
  * 普通最小二乘估计法(Ordinary Least Squares, OLS)：线性回归中最常用、最经典的系数估计方法
  * OLS对距离的定义是：残差的平方${\hat{u}_i}^2$。因此OLS估计的思想是：**OLS估计求得的系数$\hat{\beta}_{0}$、$\hat{\beta}_{1}$，将使直线与所有样本的拟合残差的平方和最小**，即
    $$
    \left(\hat{\beta}_{0}, \hat{\beta}_{1}\right)=\operatorname{argmin} \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)^{2}
    $$
  * 拟合优度
    * 回归分析中最常用的拟合优度是R方，定义为
        $$
        R^{2}=\frac{E S S}{T S S}
        $$
    * 其中
      * TSS(Total sum of squares)，总平方和
        $ T S S=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2} $

      * ESS(Explained sum of squares)，解释平方和 $ E S S=\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2} $
      * RSS(Resiual sum of squares)，残差平方和 $R S S=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}$
  * OLS估计的代数性质：OLS估计的残差与参与回归的自变量不相关
  * 经典线性模型假设下OLS估计的性质      
    * 经典线性模型假设-CLM假设，CLM假设总共有六条，在六条假设下OLS估计具有非常优良的性质
      * 总体模型假设：假定了我们正确地判断了因变量和自变量之间的关系——线性关系
      * 随机误差条件均值零假设：所有非自变量的其他因素都与自变量线性无关
      * 随机抽样假设：样本均为随机抽样样本，彼此之间相互独立
      * 非完全共线性假设：所有自变量间不能存在有完全共线性
      * 同方差假设：论$x_i$如何变化，数据与样本条件均值的偏离程度都是恒定的
      * 正态性假设：随机误差$u$的零条件期望与恒定条件方差的基础上，增加了一个服从条件正态分布的假设
    * OLS估计的性质-最优的线性无偏估计
      * 在所有无偏估计当中，OLS估计是最优的，因为有 Gauss-Markov定理：在CLM假设MLR.1-MLR.5下，在 β 的所有线性无偏估计类当中，OLS估计的方差最小
      * OLS只是在线性无偏估计中的方差最小，如果我们不追求估计的无偏性而只追求估计的稳定性（小方差），可以采用岭估计等有偏估计
      * MLR.6正态性假设成立的基础上，可以使用t分布进行模型的假设检验

#### 体会
1. 对于学渣的我来说，从 OLS估计的求解 那里开始就感觉有点难了，以至于计算细节我直接跳过了，因为我感觉我用不到这么深入的知识，实际上都是调库+调参（笑哭）
